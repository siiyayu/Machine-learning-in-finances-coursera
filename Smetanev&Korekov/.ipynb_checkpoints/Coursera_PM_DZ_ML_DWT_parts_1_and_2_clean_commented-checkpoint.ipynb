{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Кластеризация путей процесса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее в лекции мы рассмотрели, что анализировать каждый путь процесса - это дорого и нецелесообразно, лучше пути процесса группировать. <br>\n",
    "Для определения основных путей процесса (наиболее частые случаи реализации) мы используем подход, основанный на кластеризации временных рядов. <br>\n",
    "Пусть каждый случай реализации процесса - это временной ряд, каждый этап - это точка временного ряда <br>\n",
    "Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.read_csv(os.path.join(data_dir, 'dtw_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем путь процесса для одного Id<br>\n",
    "Процесс последовательно проходит через статусы: 10-20-25-30-33-35-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ts[df_ts.Id=='0894EF37C23A1ED88DC55B5D17A8294F']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Перед кластеризацией временных рядов нужно посчитать расстояние между рядами. <br>\n",
    "Например, это можно сделать через L1/L2-distance, но лучше использовать DTW-алгоритм <br>\n",
    "Имплементируем его по частям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Преобразуйте исходный набор данных:\n",
    "# - Трансформируйте датасет, чтобы каждая строка содержала (Id, Status_1...Status_n) - pandas.crosstab()\n",
    "# - Значения - это сумма Timediff \n",
    "# Не забудьте сбросить индекс в новом dataframe\n",
    "\n",
    "transformed_df = # Ваш код #\n",
    "cols = transformed_df.columns.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для примера выберем два ряда:\n",
    "x1 = transformed_df[transformed_df.Id=='645106F183B01EE88EAE80635E4F20D3'].fillna(-10).values[0,1:].astype(np.float16)\n",
    "x2 = transformed_df[transformed_df.Id=='0894EF37C23A1ED88F9307ED42AE579B'].fillna(-10).values[0,1:].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Построим их графики \n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x1.T, 'b', label='x1')\n",
    "plt.plot(x2.T, 'g', label='x2')\n",
    "ax.set_xticklabels(cols)\n",
    "ax.set_xticks([x for x in range(len(cols))])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Для расчета расстояний методом DTW нужно выполнить три шага:\n",
    "- Посчитать расстояние между рядами\n",
    "- Вычислить accumulated loss\n",
    "- Вычислить backtrack по матрице accumulated loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем L2-расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L2_dist(x1, x2):\n",
    "    distances = np.zeros((len(x1), len(x2)))\n",
    "  \n",
    "    # Реализуйте алгоритм, высчитывающий попарное L2-расстояние для всех точек двух временных рядов\n",
    "    #### Ваш код ####\n",
    "\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = L2_dist(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dims = (5, 4)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "ax.set(title='Матрица расстояний между временными рядами')\n",
    "sns.heatmap(d, vmin=0, vmax=np.max(d))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычислим accumulaed loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомню, что accumulated loss считается по формуле D<sub>i,j</sub>=d<sub>i,j</sub>+min{D<sub>i,j</sub>, D<sub>i−1,j</sub>, D<sub>i,j−1</sub>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def D_loss(dist_matrix):\n",
    "    D_loss = np.zeros((dist_matrix.shape[0], dist_matrix.shape[1]))\n",
    "    \n",
    "    #### Ваш код ####\n",
    "    \n",
    "    return D_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = D_loss(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dims = (5, 4)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "ax.set(title='Накопленный loss между рядами')\n",
    "sns.heatmap(D, vmin=0, vmax=np.max(D))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычислим DTW-path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTW-path - это путь из D<sub>M,N</sub> в D<sub>1,1</sub>. На каждом шаге переход осуществляется по элементам с минимальным accumulated loss: min{D<sub>i,j</sub>, D<sub>i−1,j</sub>, D<sub>i,j−1</sub>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DTW(accum_loss):\n",
    "    #### path - оптимальный путь\n",
    "    #### transform_cost - затраты на преобразование ряда\n",
    "    #### Ваш код ####\n",
    "    return path, transform_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path, transform_cost = DTW(x1, x2, D, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dims = (5, 4)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "ax.set(title='DTW + Накопленный loss')\n",
    "sns.heatmap(D, vmin=0, vmax=np.max(D))\n",
    "plt.plot([x[0] for x in path], [x[1] for x in path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Выбор наиболее влияющих факторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё один пример применения машинного обучения в анализе процессов - это определение факторов, которые влияют на время выполнения процесса.<br>\n",
    "Рассмотрим несколько примеров для разного класса моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испортируем полезные библиотеки и загрузим исходный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2\n",
    "import seaborn as sns\n",
    "\n",
    "features_df = pd.read_csv(os.path.join(data_dir, 'dataset.csv'), error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL - это класс поломки (всего 5 классов), его надо предсказать<br>\n",
    "все остальные поля - признаки<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на features и target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = features_df.drop('LABEL', axis=1)\n",
    "y = features_df.LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство полей - категориальные признаки, их нужно закодировать через label encoder или one-hot encoder<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_labels = ### label encoded представление ### \n",
    "X_ohe =    ### one-hot encoded encoded представление ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества модели будем использовать 20% от выборки  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_labels, y, test_size=0.2, random_state=0)\n",
    "X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe = train_test_split(X_ohe, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Обучим линейную модель <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_lg = ### Логистическая регрессия. Обратите внимание, что вы решаете задачу с несколькиими классами ###\n",
    "clf_lg.fit(X_train_ohe, y_train_ohe)\n",
    "y_pred_clf = clf_lg.predict(X_test_ohe)\n",
    "print(metrics.classification_report(y_test_ohe, y_pred_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вклад каждого фактора может быть интерпретировано как соответствующий коэффициент при признаке <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg_scores = np.append(clf_lg.intercept_, clf_lg.coef_)\n",
    "lg_df = pd.DataFrame(list(zip(X_train_ohe.columns, abs(lg_scores))), columns=['Фактор', 'Score_rf'])\n",
    "print(lg_df.sort_values('Score_rf', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, если мы используем Random Forest, то получить оценку вклада факторов можно через меру <i>mean impurity decrease</i> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf =  ### Fit Random Forest to train data ###\n",
    "\n",
    "rf_scores = clf_rf.feature_importances_\n",
    "rf = pd.DataFrame(list(zip(X.columns, rf_scores)),columns=['Фактор', 'Score_rf'])\n",
    "\n",
    "### Если необходимо, нормализуйте величины Score_rf, записанные в dataframe ###\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "### Отсортируйте датафрейм rf по Score_rf и выведите top-5 наиболее важных признаков ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с линейными моделями или tree-методами получить важность признаков довольно просто.<br>\n",
    "Но если вы используете SVM с нелинейным ядром, то результаты будут неинтерпретируемы.<br>\n",
    "В этом случае можно использовать Permutation Importance, который реализован в модуле eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(### Ваш код ###)\n",
    "perm.fit(### Ваш код ###)\n",
    "eli5.show_weights(### Ваш код ###)\n",
    "    \n",
    "### Выведите top-5 наиболее важных признаков ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
